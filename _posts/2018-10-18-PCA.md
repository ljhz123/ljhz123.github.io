---
layout: post
title:  "차원축소(1) - PCA "
date:   2018-10-18 02:54:10 +0900

---
차원축소 선형맵핑 기법중 1번째 PCA 알고리즘을 소개하겠습니다.

차원축소는 왜 하나요?

1.훈련속도의 상승 ▷ 그러나 항상 더 좋은 solution 을 제공하진 않는다.

2.데이터 시각화를 통한 패턴감지 및 통찰

## 차원축소 방법
1. Projection
2. Manifold Larning

## Mnist Data 
머신러닝을 공부하다보면 앞으로 항상 보개될 대표적 데이터셋인 Mnist 데이터를 소개하겠습니다.

![Imgur](https://i.imgur.com/TpsQ9zl.png)

손글씨를 0~255 숫자로 명암을 나타 내고 있습니다.
각 이미지는 28*28 픽셀이므로 784차원의 데이터입니다.
이 고차원 속에 유의미한 숫자 데이터는 아주 작은 공간에 있습니다.
(흰 여백이 많다는 의미입니다.)
Mnist 데이터를 두고 고차원의 임베딩 공간을 통해 스위핑 및 커브하는 낮은 차원의 매니폴드 라고 정의하기도 합니다. 
또한, 주변공간으로 부터 튀어나온 촉수같은 돌출부로 생각 할수도 있습니다.

이 데이터를 이제 2차원의 입장에서 보도록 하겠습니다.

![Imgur](https://i.imgur.com/8fmpjLA.png)

대부분의 데이터가 구별이 안되는데 빨강점인 3과 초록점인 2는 어느정도 구별이 되는것 같습니다. 
이러한 2차원의 각도를 정할때 어는 시점에서 데이터를 바라보는것이 최적의 관점인지 
이를 알아내기위해 PCA라는 알고리즘을 이용할수 있습니다.
## PCA 알고리즘

pca를 할때 투영할 초평면을 선택해야합니다. 이때 선택할 초평면을 어떻게 결정하는가?
pca알고리즘은 분산이 최대한 보존되는 축을 선택하는것이 정보가 가장 적게 손실된다 합니다.
이는 다른말로, 원본 데이터셋과 투영된것 사이의 거리를 최소화 하는 축을 뜻합니다.
(Reconstruction error 최소화 -> 이후 LLE 알고리즘 부분에서 다루도록하겠습니다.)

**투영후 분산의 최대화** 

![Imgur](https://i.imgur.com/ciXZFG7.png)

처음 분산의 최대화 라는것이 이해가 되지않아 그림으로 조금 더 알아보도록 하겠습니다.
직관적으로 오른쪽 3개의 투영데이터중 데이터의 보존이 가장 잘 된 것은 1번 그림임을 알 수 있습니다.
3번 데이터는 원 데이터의 속성이 거의 없어졌음을 알수있죠.
이는 차원 축소후 데이터의 분산값이 커야 정보 손실이 없음을 알수있습니다.

step1. 평균을 0으로 맞추기

$$
X = X-\hat { X } 
$$ 

step2. 분산값 계산

$$
v\quad =\quad ({ w }^{ T }X)({ w }^{ T }X)^{ T }=w^{ T }XX^{ T }w=nw^{ T }Sw
$$
$$ max\quad w^{ T }Sw $$
$$ s.t. w^Tw = 1 $$

이 문제를 해결해야하는데, 라그랑주 승수를 사용하여 식을 풀어 보도록하겠습니다.
$$
L\quad =\quad { w }^{ T }Sw-\lambda ({ w }^{ T }w-1)\\ \frac { \partial L }{ \partial w } \quad =\quad (S\quad +{ S }^{ T })w-2\lambda w\\ \quad \qquad \quad =2Sw-2\lambda w\\ \qquad \quad \quad =0\\ \therefore \quad Sw=\lambda w
$$

즉, 위의 식을 만족하는 w벡터가 바로 분산을 최대화 한다는 것이죠.
$$\lambda값은\quad S공분산의\quad 고유값(eigenvalue)이며\quad eigenvector로\quad 투영했을때\quad 분산이\quad 됩니다. $$ 
이때 고유벡터의 열벡터를 주성분이라 합니다. 따라서, 고유벡터에 투영하는것이 분산이 최대가 되는것입니다.

고유값을 구하기 위해서는 eigenvalue-decomposition, SVD 두가지 방법이 있습니다.
(SVD방법이 조금 더 효율적이라고 하는데.. 자세한건 SVD 방법을 포스팅때 다루겠습니다.)

```python
X_center = X-np.mean(X)
S = np.dot(X_center.T,X_center)/(X.shape[0]-1)
eig_val, eig_vecs = np.linalg.eig(S)

idx = np.random.choice(70000,500,replace=False)

reduce_dimension = 154
W = eig_vecs[:,:reduce_dimension]
new_mnist = X.dot(W)
new_mnist_sample = new_mnist[idx,:]

with plt.style.context("seaborn-darkgrid"):
    label = np.unique(y)
    for l in label:
        plt.scatter(new_mnist_sample[y[idx]==l,1], new_mnist_sample[y[idx]==l,2],label=l)
    plt.xlabel("pc1")
    plt.ylabel("pc2")
    plt.legend()
    plt.show()

```


이제 PCA로 구한 주성분으로 Mnist 데이터를 보도록합시다.
pca에서 나온 주성분축이 어떤 관점인지 시각적으로 보도록 하겠습니다.

![Imgur](https://i.imgur.com/P5iiGu4.png)


처음 임의의 2차원을 선택한것 보다 확실히 데이터의 정보가 손실 되지 않은 것을 알수있습니다.
직관적으로 0과 1은 구별이 잘 되는 것을 알수 있습니다.

그렇다면 지금 나타내는 PC1 , PC2가 어떤속성 값인지 파악해보도록 하겠습니다.
즉, PC1으로 투영했을때 PC1이 어떤 방향으로 데이터를 투영하는지 시각화를 통해 알아 보는것 입니다. 
고유벡터 또한 784차원 이므로 28*28 픽셀로 만들어 시각화 합니다.


![Imgur](https://i.imgur.com/5vTbgGU.png)![Imgur](https://i.imgur.com/iGnwRo5.png)

붉은 부분이 뜻하는것은 양의 값을 뜻하고 반대로 하늘색은 음의 값을 뜻합니다.
대부분 0값은 PC1 값에 투영시 음의 값을 가지게 되고, 반대로 1의값은 양의 값을 가지게 됩니다.
그렇기에 PC1을 통해 0/1이 구별이 잘 되는것으로 생각할수 있습니다.

이 예시를 통해 주성분벡터가 어떤 의미를 가지는지 좀더 직관적으로 파악 하셨길 바랍니다.

## 차원축소의 성능평가

차원축소가 얼마나 잘 되었나 평가하는 방법은 2가지정도 존재합니다.
1. 예측분류기에 원데이터와 축소데이터를 넣어 성능비교를 합니다.
   -> 이과정에서 성능의 차이가 크지 않아야 합니다.
2. 축소후 원래 차원으로 복원시킨후 reconstruction error 를 측정하는것입니다.
   -> 하지만 안타깝게도 모든 차원축소 알고리즘이 복원이 가능하거나 그리 쉽지는 않습니다.(pca는 쉽다)

복원 후 정보의 손실이 어느 정도인지 확인해 보도록 하겠습니다.
$$
{ w }^{ T }X\quad -->\quad w{ w }^{ T }X
$$
```python
recon_mnist = new_mnist.dot(W.T)
re_image1 = recon_mnist[500,:].real.reshape(28,28) 
ori_image1 = X[500,:].real.reshape(28,28) 
plt.imshow(ori_image1,cmap='hot', interpolation='nearest')
plt.imshow(re_image1,cmap='hot', interpolation='nearest')

```


![Imgur](https://i.imgur.com/VH84Efe.png)![Imgur](https://i.imgur.com/UsRDsZC.png)


선명도의 차이가 있을뿐 0이라는 모양의 정보는 거의 손실되지 않았습니다.
(이는 분산이 95%되는 지점까지 차원축소를 하였기 때문입니다. 즉 정보의 손실이 5%입니다)

## PCA 단점 및 한계
pca의 단점으로는 전체 데이터를 사용하여 eigen decomposition을 진행하기 때문에 새로운 데이터가 들어오면 새롭게 진행해야합니다.
이를 해결하기위해 IPCA(Incremental PCA) 또는 Online PCA 라고 하는 알고리즘을 사용가능합니다.
(sklearn 알고리즘 참조)

non linear 데이터(스위스롤) 같은 데이터에 pca를 적용시 좋은 효과를 볼수없습니다. 
kernel PCA 같은 기법은 이를 해결해줄수있습니다.

randomize PCA 는 차원의 수를 많이 줄이거나, 쇼핑Item 데이터 처럼 n*d에서 d가 매우 큰 데이터셋에서 PCA보다 좋은 성능을 나타낸다고합니다.

## NEXT
앞서 차원 축소의 종류중 2번째 매니폴드 학습에 대해 알아보고, 기본이되는 알고리즘인 MDS를 소개하도록 하겠습니다.

[차원축소(2) - MDS](https://ljhz123.github.io/2018/10/18/MDS.html)